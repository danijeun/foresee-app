"""
Target Variable Agent for Foresee App
Uses Gemini LLM to suggest optimal target variables for ML analysis
"""
import sys
from pathlib import Path
import pandas as pd
import google.generativeai as genai
from typing import Dict, List, Any

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from services.workflow_manager import WorkflowManager
from services.config import Config
import os


class TargetVariableAgent:
    """
    Agent that analyzes dataset structure and suggests optimal target variables
    using Gemini LLM for intelligent recommendations
    """
    
    def __init__(self, gemini_api_key: str = None):
        """
        Initialize the Target Variable Agent
        
        Args:
            gemini_api_key: Google Gemini API key (uses env variable if not provided)
        """
        # Configure Gemini API
        api_key = gemini_api_key or os.getenv('GEMINI_API_KEY')
        if not api_key:
            raise ValueError("GEMINI_API_KEY must be provided or set in environment")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-pro')
        
        # Initialize workflow manager
        self.workflow_manager = None
        
    def analyze_workflow(self, workflow_id: str, table_name: str = None, 
                        sample_size: int = 100) -> Dict[str, Any]:
        """
        Analyze a workflow's data and suggest target variables
        
        Args:
            workflow_id: UUID of the workflow
            table_name: Name of the table to analyze (if None, uses the first table found)
            sample_size: Number of rows to sample for analysis
            
        Returns:
            dict: Analysis results with suggested target variables and reasoning
        """
        try:
            # Initialize workflow manager
            self.workflow_manager = WorkflowManager()
            
            # Get uploader for this workflow
            uploader = self.workflow_manager.get_workflow_uploader(workflow_id=workflow_id)
            schema_name = f"WORKFLOW_{workflow_id}"
            
            # If no table name provided, find the first data table
            if table_name is None:
                table_name = self._find_data_table(uploader, schema_name)
                if not table_name:
                    raise ValueError(f"No data tables found in workflow {workflow_id}")
            
            print(f"ğŸ” Analyzing table: {table_name}")
            
            # Fetch sample data
            query = f"SELECT * FROM {table_name} LIMIT {sample_size}"
            sample_data = uploader.query(query)
            column_names = [desc[0] for desc in uploader.cursor.description]
            
            # Get data types and statistics
            stats = self._get_column_statistics(uploader, table_name, column_names)
            
            # Create DataFrame for easier analysis
            df = pd.DataFrame(sample_data, columns=column_names)
            
            # Generate data profile
            data_profile = self._create_data_profile(df, stats)
            
            # Use Gemini to suggest target variables
            suggestions = self._get_gemini_suggestions(data_profile)
            
            # Clean up
            uploader.close()
            
            return {
                'success': True,
                'workflow_id': workflow_id,
                'table_name': table_name,
                'schema_name': schema_name,
                'columns': column_names,
                'row_count': stats['total_rows'],
                'data_profile': data_profile,
                'suggestions': suggestions
            }
            
        except Exception as e:
            print(f"âŒ Error analyzing workflow: {str(e)}")
            raise
        finally:
            if self.workflow_manager:
                self.workflow_manager.close()
    
    def _find_data_table(self, uploader, schema_name: str) -> str:
        """
        Find the first data table in the workflow schema (excluding metadata tables)
        """
        try:
            uploader.cursor.execute(f"SHOW TABLES IN SCHEMA {schema_name}")
            tables = uploader.cursor.fetchall()
            
            # Filter out metadata tables
            for table in tables:
                table_name = table[1]  # Table name is in column 1
                if table_name != 'WORKFLOW_METADATA':
                    return table_name
            
            return None
        except Exception as e:
            print(f"âš ï¸ Error finding tables: {e}")
            return None
    
    def _get_column_statistics(self, uploader, table_name: str, 
                               column_names: List[str]) -> Dict[str, Any]:
        """
        Get statistical information about the columns
        """
        stats = {'columns': {}}
        
        # Get total row count
        uploader.cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        stats['total_rows'] = uploader.cursor.fetchone()[0]
        
        # Get statistics for each column
        for col in column_names:
            col_stats = {}
            
            # Count nulls
            uploader.cursor.execute(f'SELECT COUNT(*) FROM {table_name} WHERE "{col}" IS NULL')
            col_stats['null_count'] = uploader.cursor.fetchone()[0]
            
            # Count distinct values
            uploader.cursor.execute(f'SELECT COUNT(DISTINCT "{col}") FROM {table_name}')
            col_stats['distinct_count'] = uploader.cursor.fetchone()[0]
            
            stats['columns'][col] = col_stats
        
        return stats
    
    def _create_data_profile(self, df: pd.DataFrame, stats: Dict) -> Dict[str, Any]:
        """
        Create a comprehensive data profile for LLM analysis
        """
        profile = {
            'total_rows': stats['total_rows'],
            'total_columns': len(df.columns),
            'columns': []
        }
        
        for col in df.columns:
            col_info = {
                'name': col,
                'dtype': str(df[col].dtype),
                'null_count': stats['columns'][col]['null_count'],
                'null_percentage': round(stats['columns'][col]['null_count'] / stats['total_rows'] * 100, 2),
                'distinct_count': stats['columns'][col]['distinct_count'],
                'sample_values': df[col].dropna().head(5).tolist()
            }
            
            # Add numeric statistics if applicable
            if pd.api.types.is_numeric_dtype(df[col]):
                col_info.update({
                    'min': float(df[col].min()) if not pd.isna(df[col].min()) else None,
                    'max': float(df[col].max()) if not pd.isna(df[col].max()) else None,
                    'mean': float(df[col].mean()) if not pd.isna(df[col].mean()) else None,
                    'std': float(df[col].std()) if not pd.isna(df[col].std()) else None
                })
            
            profile['columns'].append(col_info)
        
        return profile
    
    def _get_gemini_suggestions(self, data_profile: Dict) -> Dict[str, Any]:
        """
        Use Gemini LLM to suggest target variables based on data profile
        """
        # Create a detailed prompt for Gemini
        prompt = self._create_analysis_prompt(data_profile)
        
        try:
            # Call Gemini API
            response = self.model.generate_content(prompt)
            
            # Parse the response
            suggestions = self._parse_gemini_response(response.text, data_profile)
            
            return suggestions
            
        except Exception as e:
            print(f"âš ï¸ Error calling Gemini API: {e}")
            return {
                'error': str(e),
                'fallback_suggestions': self._get_fallback_suggestions(data_profile)
            }
    
    def _create_analysis_prompt(self, data_profile: Dict) -> str:
        """
        Create a detailed prompt for Gemini LLM with emphasis on importance ranking
        """
        columns_info = "\n".join([
            f"- {col['name']}: {col['dtype']}, {col['distinct_count']} unique values, "
            f"{col['null_percentage']}% null, samples: {col['sample_values'][:3]}"
            for col in data_profile['columns']
        ])
        
        prompt = f"""
You are a machine learning expert analyzing a dataset to identify and rank the MOST IMPORTANT target variables for predictive modeling.

Dataset Overview:
- Total rows: {data_profile['total_rows']}
- Total columns: {data_profile['total_columns']}

Columns:
{columns_info}

TASK: Identify the most important target variables ranked by their importance for machine learning.

Consider these factors when ranking importance:
1. Business/Analytical Value: Which variables would provide the most valuable predictions?
2. Data Quality: Low null percentages, appropriate cardinality
3. Predictability: Sufficient features available to predict this target
4. Problem Clarity: Clear regression or classification task
5. Actionability: Predictions that can drive decisions

Please provide the TOP 5 most important target variables, RANKED FROM MOST TO LEAST IMPORTANT.

For each recommendation, provide:
- Importance Score (1-100): How important this target is for ML modeling
- Variable Name: The column name
- Problem Type: regression/binary classification/multi-class classification
- Why Important: Clear explanation of business/analytical value
- Predictability: Assessment of how well this can be predicted
- Suggested Features: Best columns to use as features
- Considerations: Any warnings or challenges

Format your response EXACTLY as follows:

RECOMMENDATION 1 (MOST IMPORTANT):
Variable: [column name]
Importance Score: [1-100]
Problem Type: [regression/binary classification/multi-class classification]
Why Important: [clear explanation of business value and why this is the MOST important target]
Predictability: [HIGH/MEDIUM/LOW - assessment of prediction feasibility]
Suggested Features: [comma-separated list of column names]
Considerations: [any warnings or notes]

RECOMMENDATION 2:
Variable: [column name]
Importance Score: [1-100]
Problem Type: [regression/binary classification/multi-class classification]
Why Important: [explanation]
Predictability: [HIGH/MEDIUM/LOW]
Suggested Features: [comma-separated list]
Considerations: [any warnings]

RECOMMENDATION 3:
[same format]

RECOMMENDATION 4:
[same format]

RECOMMENDATION 5:
[same format]

RANKING RATIONALE:
[Brief explanation of why you ranked these variables in this specific order]
"""
        return prompt
    
    def _parse_gemini_response(self, response_text: str, 
                               data_profile: Dict) -> Dict[str, Any]:
        """
        Parse Gemini's response into structured format with importance scoring
        """
        recommendations = []
        ranking_rationale = ""
        
        # Split response into sections
        sections = response_text.split('RECOMMENDATION')
        
        # Parse each recommendation
        for i, section in enumerate(sections[1:], 1):  # Skip first empty section
            if i > 5:  # Only process top 5
                break
                
            rec = {
                'rank': i,
                'variable': None,
                'importance_score': None,
                'problem_type': None,
                'why_important': None,
                'predictability': None,
                'suggested_features': [],
                'considerations': None
            }
            
            lines = section.strip().split('\n')
            current_field = None
            field_content = []
            
            for line in lines:
                line_stripped = line.strip()
                
                # Check if this is a new field
                if line_stripped.startswith('Variable:'):
                    if current_field and field_content:
                        rec[current_field] = ' '.join(field_content).strip()
                    rec['variable'] = line_stripped.replace('Variable:', '').strip()
                    current_field = None
                    field_content = []
                    
                elif line_stripped.startswith('Importance Score:'):
                    if current_field and field_content:
                        rec[current_field] = ' '.join(field_content).strip()
                    score_text = line_stripped.replace('Importance Score:', '').strip()
                    try:
                        # Extract numeric score
                        import re
                        score_match = re.search(r'\d+', score_text)
                        if score_match:
                            rec['importance_score'] = int(score_match.group())
                    except:
                        rec['importance_score'] = None
                    current_field = None
                    field_content = []
                    
                elif line_stripped.startswith('Problem Type:'):
                    if current_field and field_content:
                        rec[current_field] = ' '.join(field_content).strip()
                    rec['problem_type'] = line_stripped.replace('Problem Type:', '').strip()
                    current_field = None
                    field_content = []
                    
                elif line_stripped.startswith('Why Important:'):
                    if current_field and field_content:
                        rec[current_field] = ' '.join(field_content).strip()
                    content = line_stripped.replace('Why Important:', '').strip()
                    current_field = 'why_important'
                    field_content = [content] if content else []
                    
                elif line_stripped.startswith('Predictability:'):
                    if current_field and field_content:
                        rec[current_field] = ' '.join(field_content).strip()
                    rec['predictability'] = line_stripped.replace('Predictability:', '').strip()
                    current_field = None
                    field_content = []
                    
                elif line_stripped.startswith('Suggested Features:'):
                    if current_field and field_content:
                        rec[current_field] = ' '.join(field_content).strip()
                    features_text = line_stripped.replace('Suggested Features:', '').strip()
                    rec['suggested_features'] = [f.strip() for f in features_text.split(',') if f.strip()]
                    current_field = None
                    field_content = []
                    
                elif line_stripped.startswith('Considerations:'):
                    if current_field and field_content:
                        rec[current_field] = ' '.join(field_content).strip()
                    content = line_stripped.replace('Considerations:', '').strip()
                    current_field = 'considerations'
                    field_content = [content] if content else []
                    
                elif line_stripped and not line_stripped.startswith('RECOMMENDATION') and not line_stripped.startswith('RANKING'):
                    # This is continuation of previous field
                    if current_field:
                        field_content.append(line_stripped)
            
            # Save any remaining field content
            if current_field and field_content:
                rec[current_field] = ' '.join(field_content).strip()
            
            if rec['variable']:  # Only add if we found a variable
                recommendations.append(rec)
        
        # Extract ranking rationale
        if 'RANKING RATIONALE:' in response_text:
            ranking_rationale = response_text.split('RANKING RATIONALE:')[-1].strip()
        
        return {
            'recommendations': recommendations,
            'ranking_rationale': ranking_rationale,
            'raw_response': response_text,
            'total_recommendations': len(recommendations)
        }
    
    def _get_fallback_suggestions(self, data_profile: Dict) -> List[Dict]:
        """
        Provide basic suggestions with importance ranking if Gemini API fails
        """
        suggestions = []
        
        for col in data_profile['columns']:
            # Calculate a basic importance score based on data quality
            importance = 50  # Base score
            
            # Reduce score for high null percentage
            if col['null_percentage'] < 5:
                importance += 20
            elif col['null_percentage'] < 20:
                importance += 10
            elif col['null_percentage'] > 50:
                importance -= 20
            
            # Look for numeric columns with reasonable cardinality for regression
            if 'int' in col['dtype'] or 'float' in col['dtype']:
                if col['distinct_count'] > 10:  # Continuous variable
                    importance += 15
                    suggestions.append({
                        'rank': 0,  # Will be assigned later
                        'variable': col['name'],
                        'importance_score': min(100, importance),
                        'problem_type': 'regression',
                        'why_important': 'Numeric column with high cardinality and good data quality - suitable for regression',
                        'predictability': 'MEDIUM',
                        'suggested_features': [c['name'] for c in data_profile['columns'] if c['name'] != col['name']][:5],
                        'considerations': f'{col["null_percentage"]}% null values - may need imputation'
                    })
                elif col['distinct_count'] <= 10 and col['distinct_count'] > 1:  # Could be classification
                    importance += 10
                    suggestions.append({
                        'rank': 0,
                        'variable': col['name'],
                        'importance_score': min(100, importance),
                        'problem_type': f'{"binary" if col["distinct_count"] == 2 else "multi-class"} classification',
                        'why_important': f'Categorical variable with {col["distinct_count"]} classes - suitable for classification',
                        'predictability': 'MEDIUM',
                        'suggested_features': [c['name'] for c in data_profile['columns'] if c['name'] != col['name']][:5],
                        'considerations': f'{col["null_percentage"]}% null values'
                    })
        
        # Sort by importance score and assign ranks
        suggestions.sort(key=lambda x: x['importance_score'], reverse=True)
        for i, sug in enumerate(suggestions[:5], 1):
            sug['rank'] = i
        
        return suggestions[:5]  # Return top 5


# ============================================
# USAGE EXAMPLE
# ============================================

if __name__ == "__main__":
    """
    Example usage of the Target Variable Agent
    """
    import json
    
    # Initialize agent
    agent = TargetVariableAgent()
    
    # Example: Analyze a workflow
    # Replace with your actual workflow_id from an upload
    workflow_id = "your_workflow_id_here"
    
    try:
        print("ğŸ¤– Starting Target Variable Analysis...")
        print("=" * 70)
        
        # Analyze the workflow
        result = agent.analyze_workflow(
            workflow_id=workflow_id,
            sample_size=100
        )
        
        # Display results
        print(f"\nâœ… Analysis Complete!")
        print(f"ğŸ“Š Dataset: {result['table_name']}")
        print(f"ğŸ“ˆ Total Rows: {result['row_count']:,}")
        print(f"ğŸ“‹ Total Columns: {len(result['columns'])}")
        print(f"\nColumns: {', '.join(result['columns'])}")
        
        # Display recommendations
        print("\nğŸ¯ Target Variable Recommendations (Ranked by Importance):")
        print("=" * 70)
        
        for rec in result['suggestions']['recommendations']:
            print(f"\n#{rec['rank']} - {rec['variable']}")
            if rec.get('importance_score'):
                print(f"   â­ Importance Score: {rec['importance_score']}/100")
            print(f"   ğŸ“Š Type: {rec['problem_type']}")
            if rec.get('predictability'):
                print(f"   ğŸ¯ Predictability: {rec['predictability']}")
            if rec.get('why_important'):
                print(f"   ğŸ’¡ Why Important: {rec['why_important']}")
            if rec.get('suggested_features'):
                print(f"   ğŸ”§ Features: {', '.join(rec['suggested_features'][:5])}")
            if rec.get('considerations'):
                print(f"   âš ï¸ Notes: {rec['considerations']}")
        
        print("\n" + "=" * 70)
        print("ğŸ“‹ Ranking Rationale:")
        print("=" * 70)
        if result['suggestions'].get('ranking_rationale'):
            print(result['suggestions']['ranking_rationale'])
        else:
            print("Variables ranked by importance score and data quality.")
        
        # Save full results to JSON
        with open('target_analysis_results.json', 'w') as f:
            json.dump(result, f, indent=2)
        print("\nğŸ’¾ Full results saved to: target_analysis_results.json")
        
    except Exception as e:
        print(f"\nâŒ Error: {str(e)}")
        import traceback
        traceback.print_exc()

